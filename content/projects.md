## Multimodal Adapter

Multimodal adapters like MMA enhance vision-language models by introducing learnable components that bridge the vision and text encoders through shared projection layers. This allows both modalities to be adapted jointly rather than tuning only one branch. Current research in this area, including methods like MaPLe, MMRL, and HiCroPL, typically applies these adapters uniformly across all layers or uses fixed architectural patterns with manually tuned hyperparameters to determine where adapters should be placed, operating under the assumption that all layers benefit equally from adaptation or following heuristic rules about early versus late layer behavior. My research aims to move beyond these uniform and heuristic approaches by leveraging influence functions to perform a principled, data-driven analysis of each layer's cross-modal dynamics and semantic characteristics, thereby identifying precisely which layers exhibit modality misalignment or inappropriate information flow and selectively inserting adapters only where they provide maximum benefit, resulting in more parameter-efficient adaptation with better generalization while eliminating the need for manual hyperparameter tuning of layer selection.

## Modality Gap and Modality Alignment

The modality gap in vision-language models refers to the geometric separation between vision and text representations. It is shown that semantically similar concepts remain distant in the joint embedding space which limits cross-modal understanding. Traditional methods address this gap only at the final projection layer via contrastive learning, but recent work shows that misalignment accumulates throughout the network’s depth. My research tackles this issue at a layer-wise level, using influence functions to quantify how individual training samples affect alignment in each layer. This analysis reveals which layers and samples worsen or improve modality alignment, identifying where the two encoders diverge in their learning dynamics. By correlating influence patterns across layers, we can detect incompatible processing between modalities and selectively insert adapters at misaligned layers. This approach also highlights which training data enhance or harm alignment, informing both model design and data curation. Finally, tracing sample-level influence across depth exposes whether misalignment arises from low-level feature issues or high-level semantic errors, providing a diagnostic tool for understanding and reducing modality gaps.

## Medical Imaging and Medical VLM

In medical imaging, large vision-language models (LVLMs) typically pair a pre-trained image encoder with a projection layer that feeds visual features into a language model for diagnosis or clinical reasoning. However, this standard architecture struggles with the fine-grained, subtle features critical to medical interpretation—such as early lesions, textural variations, and small anatomical abnormalities—that may be lost when transferring representations from vision to language spaces. Evidence shows that LVLMs often underperform compared to simpler CLIP-based encoders on medical tasks, suggesting that language modeling and projection steps can degrade essential visual information. My research examines these architectural bottlenecks using influence functions to trace how individual training samples—especially those with subtle pathology—affect each layer of the encoder and projection components. This layer-wise analysis identifies where medical information is lost or distorted, whether through insufficient visual encoding, projection bottlenecks, or incompatibility with the language model’s feature space. Influence patterns also reveal which image types (e.g., early- vs. late-stage findings) are most vulnerable to degradation, uncovering biases in how LVLMs process varying levels of clinical complexity. Ultimately, this work investigates whether adaptive, layer-wise projection or selective feature preservation strategies can better maintain diagnostic fidelity, testing whether current LVLM architectures are sufficient for medicine or require domain-specific redesigns to meet clinical demands.

## Preference Optimization

In large language model alignment, Direct Preference Optimization (DPO) has emerged as an efficient alternative to RLHF, directly training models to favor human-approved responses without a reward model. Current research follows two main paths: improving domain-specific preference datasets (e.g., for medicine, law, or coding) and refining the algorithmic granularity of alignment. Recent extensions like Token-level DPO (TDPO) and Token-Guided DPO (TGDPO) recognize that not all tokens in a response equally contribute to quality—rejected answers may include useful reasoning, while preferred ones may contain noise—motivating finer token-level credit assignment. My research advances this direction by introducing influence functions to analyze how individual tokens and preference pairs shape aligned behavior. Token-wise influence scores reveal which parts of responses most strongly affect learning, exposing noisy or misleading examples and testing whether token-level optimization targets meaningful reasoning differences or superficial lexical patterns. This method also uncovers failure modes, such as penalizing correct technical terms or rewarding uninformative stylistic cues.

## Bangla NLP

I am actively working on Bangla NLP and multimodal learning, focusing on understanding how large language and vision-language models (LLMs/LMMs) perform on Bangla tasks and where their limitations lie. Despite Bangla being one of the most widely spoken languages globally, research and high-quality resources for Bangla NLP remain limited, leading to significant performance disparities compared to English-centric models. My current work systematically evaluates the capability of LLMs and LMMs in solving core Bangla tasks, particularly in multimodal and socially impactful settings such as meme detection, hate content identification, visual question answering (VQA), and medical VQA. Beyond benchmarking accuracy, I aim to uncover the underlying reasons behind model failures—whether they stem from linguistic complexity, data scarcity, or cross-modal misalignment—providing insights into how large models process Bangla text and multimodal inputs. Ultimately, this research seeks to advance explainable and equitable Bangla NLP, guiding the development of culturally and linguistically adaptive foundation models for low-resource yet globally significant languages.
