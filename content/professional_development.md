<h2>Selected Publications</h2>

<h3>1. BanglaTLit: A Benchmark Dataset for Back-Transliteration of Romanized Bangla </h3> <i> EMNLP-2024 [Findings] </i> <a href="https://aclanthology.org/2024.findings-emnlp.859.pdf">[Paper]</a>

<i> Authors - </i> <b>Md Fahim</b>\*, Fariha Tanjim Shifat\*, Fabiha Haider\*, Deeparghya Dutta Barua, Md Sakib Ul Rahman Sourove, Md Farhan Ishmam, Farhad Alam Bhuiyan

<!-- <br>
<body>
<button class="transparent-button" onclick="window.location.href='https://aclanthology.org/2024.findings-emnlp.859.pdf'"> -->
    
<!-- </button> </body>
<body>
<button class="transparent-button" onclick="window.location.href='https://github.com/farhanishmam/BanglaTLit'">
    <a href="https://github.com/farhanishmam/BanglaTLit">Code</a>
</button> </body> -->

<!-- <div style="display: flex; justify-content: center; align-items: center; margin-top: 10px; margin-bottom: 10px; width: 100%;">
    <img src="images/tLitOverview.PNG" style="border: 2px solid black; max-width: 38%; height: 76%; margin-right: 10px;">
    <img src="images/tLit.png" style="border: 2px solid black; max-width: 60%; height: auto;">
</div>

<ul style="font-size: 16px;">
    <li> First large-scale automated Bangla transliteration, BanglaTLit, with over 42.7k samples.</li>
    <li> A romanized Bangla pre-training corpus, BanglaTLit-PT, with over 245.7k samples. </li>
    <li> Novel T5-based dual encoder architecture achieving SOTA on BanglaTLit.</li>
</ul> -->

<h3>2. Improving the Performance of Transformer-based Models Over Classical Baselines in Multiple Transliterated Languages</h3> <i> ECAI-2024 <a href="https://www.ecai2024.eu/programme/schedule" style="color: darkred;">[Full Talk Presentation] </a> </i> <a href="https://ebooks.iospress.nl/doi/10.3233/FAIA240972">[Paper]</a>

<i> Authors - </i>Fahim Ahmed\*, <b>Md Fahim</b>\*, Amin Ahsan Ali, Ashraful Amin, AKM Mahabubur Rahman
<!-- <body>
<button class="transparent-button" onclick="window.location.href='https://ebooks.iospress.nl/doi/10.3233/FAIA240972'">
    <a href="https://ebooks.iospress.nl/doi/10.3233/FAIA240972">Paper</a>
</button> </body> -->

<h3>3. BanTH: A Multi-label Hate Speech Detection Dataset for Transliterated Bangla
</h3> <i>  NAACL-2025 [Findings] </i> <a href="https://arxiv.org/abs/2410.13281">Paper [Preprint]</a>

<i> Authors - </i>Fabiha Haider, Fariha Tanjim Shifat, Md Farhan Ishmam, Deeparghya Dutta Barua, Md Sakib Ul Rahman Sourove, <b>Md Fahim</b>\*, Md Farhad Alam
<!-- <body>
<button class="transparent-button" onclick="window.location.href='https://arxiv.org/abs/2410.13281'">
    <a href="https://arxiv.org/abs/2410.13281">Paper [Preprint]</a>
</button> </body> -->

<h3>4. HateXplain Space Model: Fusing Robustness with Explainability in Hate Speech Analysis</h3> 
<i> ENLSP Workshop @ NeurIPS </i> <a href="https://neurips2023-enlsp.github.io/papers/paper_91.pdf">[Paper]</a>

<i> Authors - </i><b>Md Fahim</b>,Md Shihab Shahriar, Mohammad Ruhul Amin

<!-- <br>
<body>
<button class="transparent-button" onclick="window.location.href='https://neurips2023-enlsp.github.io/papers/paper_91.pdf'">
    <a href="https://neurips2023-enlsp.github.io/papers/paper_91.pdf">Paper</a>
</button> 
</body> -->

<h3>5. Aambela at BLP-2023 Task 2: Enhancing BanglaBERT Performance for Bangla Sentiment Analysis Task with In Task Pretraining and Adversarial Weight Perturbation</h3>
<i> BLP Workshop @ EMNLP<a href="https://blp-workshop.github.io/" style="color: darkred;"> [Best Paper Award] </a> </i> <a href="https://aclanthology.org/2023.banglalp-1.42.pdf">[Paper]</a>

<i> Authors - </i><b>Md Fahim</b>

<!-- <br>
<body>
<button class="transparent-button" onclick="window.location.href='https://aclanthology.org/2023.banglalp-1.42.pdf'">
    <a href="https://aclanthology.org/2023.banglalp-1.42.pdf">Paper</a>
</button> 
</body> -->

<h3>6. TinyLLM Efficacy in Low-Resource Language: An Experiment on Bangla Text Classification Task</h3>
<i> ICPR 2024<a href="https://icpr2024.org/pdf/Oral%20Papers.pdf" style="color: darkred;"> [Oral Presentation] </a> </i> <a href="https://link.springer.com/chapter/10.1007/978-3-031-78495-8_30">[Paper]</a>

<i> Authors - </i>Farhan Noor Dehan*, <b>Md Fahim</b>\*, Amin Ahsan Ali, Ashraful Amin, AKM Mahabubur Rahman

<!-- <br>
<body>
<button class="transparent-button" onclick="window.location.href='https://link.springer.com/chapter/10.1007/978-3-031-78495-8_30'">
    <a href="https://link.springer.com/chapter/10.1007/978-3-031-78495-8_30">Paper</a>
</button> 
</body> -->

<h3>7. ChitroJera: A Regionally Relevant Visual Question Answering Dataset for Bangla</h3> <a href="https://arxiv.org/abs/2410.14991">[Paper]</a>

<i> Authors - </i> Deeparghya Dutta Barua\*, Md Sakib Ul Rahman Sourove\*, Md Farhan Ishmam\*, Fabiha Haider, Fariha Tanjim Shifat, <b>Md Fahim</b>, Farhad Alam Bhuiyan

<!-- <br>
<body>
<button class="transparent-button" onclick="window.location.href='https://arxiv.org/abs/2410.14991'">
    <a href="https://arxiv.org/abs/2410.14991">Paper</a>
</button> 
<button class="transparent-button" onclick="window.location.href='https://github.com/farhanishmam/ChitroJera'">
    <a href="https://github.com/farhanishmam/ChitroJera">Code</a>
</button>  -->

<!-- </body> -->

<!-- A full code for completed paper (Paper + Code)
<h3>From Image to Language: A Critical Analysis of Visual Question Answering (VQA) Approaches, Challenges, and Opportunities</h3>

<b>Md Farhan Ishmam</b>, Md Sakib Hossain Shovon, Muhammad Firoz Mridha, Nilanjan Dey <br>
<body>
<button class="transparent-button" onclick="window.location.href='https://www.sciencedirect.com/science/article/abs/pii/S1566253524000484'">
    <a href="https://www.sciencedirect.com/science/article/abs/pii/S1566253524000484"> Information Fusion 2024 </a>
</button>

</body>

<div style="display: flex; justify-content: center; align-items: center; margin-top: 10px; margin-bottom: 10px; width: 100%;">
    <img src="images/vqaOverview.PNG" style="border: 2px solid black; max-width: 50%; height: 85%;  margin-right: 10px;">
    <img src="images/vqaApp.PNG" style="border: 2px solid black; max-width: 50%; height: auto;">
</div>

<ul style="font-size: 16px;">
    <li> Comprehensive survey on VQA datasets, methods, metrics, challenges, and research opportunities. </li>
    <li> New taxonomy that systematically categorizes VQA literature and multimodal learning tasks. </li>
    <li> Novel real-world applications of VQA in domains e.g. assistive technology, education, and healthcare. </li>
</ul>
-->